{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Policy TD Control (SARSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.79| 0.87| 0.83| 0.00|\n",
      "---------------------------\n",
      " 0.70| 0.00|-0.92| 0.00|\n",
      "---------------------------\n",
      " 0.62|-0.51|-0.73|-0.96|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # we'll use epsilon-soft to ensure all states are visited\n",
    "  # what happens if you don't do this? i.e. eps=0\n",
    "  p = np.random.random()\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding rewards (not returns as in MC)\n",
    "  # start at the designated start state\n",
    "  s = (2, 0)\n",
    "  grid.set_state(s)\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    a = random_action(a)\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  return states_and_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  for it in range(1000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_rewards = play_game(grid, policy)\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    for t in range(len(states_and_rewards) - 1):\n",
    "      s, _ = states_and_rewards[t]\n",
    "      s2, r = states_and_rewards[t+1]\n",
    "      # we will update V(s) AS we experience the episode\n",
    "      V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy TD Control (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFSxJREFUeJzt3X2wXPV93/H315IlHmwLkGUb8xCJASeVk7QhtzKkietCw4PTonQqzwhnYqWhQ5yWGbe0Y4vx1C7EnRbHY9KM1cZKISWmCSKyE6uArRDjSVMXy7oCAshY5iKermWbC1JkCyz09O0f+7tktezVnqu7V7v3nPdrZuee8zu/c/b7u0f67Llnz+6JzESS1AyvG3QBkqQTx9CXpAYx9CWpQQx9SWoQQ1+SGsTQl6QGMfQlqUEMfUlqEENfkhpk/qAL6PTmN785ly5dOugyJGlO2bZt2wuZuaRXv6EL/aVLlzI6OjroMiRpTomIZ6r08/SOJDWIoS9JDWLoS1KDGPqS1CCGviQ1SKXQj4grImJHRIxFxNouy98dEQ9GxKGIWNWxbE1EPFEea/pVuCRp+nqGfkTMA9YBVwLLgasjYnlHt2eBXwP+qGPdM4CPA+8CVgAfj4jTZ162JOl4VDnSXwGMZebOzDwA3AmsbO+QmU9n5iPAkY51Lwfuy8zdmbkHuA+4og91v8b39u5n3VfH2PfKodnYvCTVQpXQPwt4rm1+vLRVUWndiLg2IkYjYnRiYqLipo/20LN7+O3NO/ja2AvHtb4kNUGV0I8ubVXvpl5p3cxcn5kjmTmyZEnPTxF3tWzJqQAcPuKN3iVpKlVCfxw4p23+bGBXxe3PZN1pifL6kma+JE2pSuhvBS6IiGURsQBYDWyquP3NwGURcXp5A/ey0tZ3Uf6myMp/hEhS8/QM/cw8BFxHK6wfB+7KzO0RcVNEXAUQEX8/IsaB9wGfjYjtZd3dwG/ReuHYCtxU2vpu8jySR/qSNLVK37KZmfcC93a0faxteiutUzfd1r0NuG0GNVYS3d49kCQdpXafyPVAX5KmVqPQn3wj19iXpKnUJvQ9vSNJvdUn9MtPD/QlaWr1Cf1yqO8lm5I0tfqE/qALkKQ5oDahP8nTO5I0tdqE/qufyDX0JWlK9Qn9yUs2B1yHJA2z+oT+q0f6xr4kTaU2oT/JyJekqdUm9F/9cJapL0lTqlHoe9GmJPVSm9Cf5IezJGlqtQn9yeP8T355x0DrkKRhVp/QL6n/4ksHBluIJA2x+oS+X8QgST3VJ/TNfEnqqT6hP+gCJGkOqE3oS5J6q0/oe6gvST3VJvR9I1eSeqtP6Jv5ktRTfUJ/0AVI0hxQn9D3UF+SeqpP6A+6AEmaA2oT+pKk3moT+p7dkaTe6hP6nuCRpJ5qE/pmviT1Vin0I+KKiNgREWMRsbbL8oURsaEs3xIRS0v76yPi9oh4NCIej4gb+lt+ew2ztWVJqo+eoR8R84B1wJXAcuDqiFje0e0aYE9mng/cAtxc2t8HLMzMnwJ+FviNyReEfjPzJam3Kkf6K4CxzNyZmQeAO4GVHX1WAreX6Y3ApdG6cD6BUyNiPnAycAD4QV8qlyRNW5XQPwt4rm1+vLR17ZOZh4C9wGJaLwAvAd8FngU+lZm7Z1hzV344S5J6qxL63dK08+7jU/VZARwG3g4sA/5dRJz3mieIuDYiRiNidGJiokJJ1YqUJB2tSuiPA+e0zZ8N7JqqTzmVswjYDbwf+HJmHszM54GvASOdT5CZ6zNzJDNHlixZMv1R4Bu5klRFldDfClwQEcsiYgGwGtjU0WcTsKZMrwLuz8ykdUrnkmg5FbgI+FZ/Sj+a1+lLUm89Q7+co78O2Aw8DtyVmdsj4qaIuKp0uxVYHBFjwPXA5GWd64A3AI/RevH4g8x8pM9jADzSl6Qq5lfplJn3Avd2tH2sbXo/rcszO9fb161dkjQY9flEriSpp9qEvqd3JKm3+oS+b+RKUk/1CX0zX5J6qk/oD7oASZoD6hP6HupLUk/1Cf1BFyBJc0BtQl+S1FttQt+zO5LUW41C39SXpF5qE/rtHnjyxUGXIElDqZahf/cjnd/8LEmCmoa+JKk7Q1+SGsTQl6QGMfQlqUEMfUlqkFqGfg66AEkaUrUMfUlSd4a+JDWIoS9JDWLoS1KDGPqS1CCGviQ1SC1D/5kXXxp0CZI0lGoZ+l8b86uVJambWoa+JKk7Q1+SGsTQl6QGMfQlqUEMfUlqkEqhHxFXRMSOiBiLiLVdli+MiA1l+ZaIWNq27Kcj4oGI2B4Rj0bESf0rX5I0HT1DPyLmAeuAK4HlwNURsbyj2zXAnsw8H7gFuLmsOx+4A/hgZr4TeA9wsG/VS5KmpcqR/gpgLDN3ZuYB4E5gZUeflcDtZXojcGlEBHAZ8Ehm/jVAZr6YmYf7U7okabqqhP5ZwHNt8+OlrWufzDwE7AUWA+8AMiI2R8SDEfHhbk8QEddGxGhEjE5MTEx3DJKkiqqEfnRp67w51VR95gM/D/xK+fnPIuLS13TMXJ+ZI5k5smTJkgolSZKOR5XQHwfOaZs/G9g1VZ9yHn8RsLu0/2VmvpCZLwP3AhfOtGhJ0vGpEvpbgQsiYllELABWA5s6+mwC1pTpVcD9mZnAZuCnI+KU8mLwD4Fv9qd0SdJ09Qz9co7+OloB/jhwV2Zuj4ibIuKq0u1WYHFEjAHXA2vLunuAT9N64XgYeDAz7+n/MF7rG0/tPhFPI0lzyvwqnTLzXlqnZtrbPtY2vR943xTr3kHrss0Tatff/OhEP6UkDT0/kStJDWLoS1KDGPqS1CCGviQ1iKEvSQ1i6EtSgxj6ktQghr4kNYihL0kNUtvQ3/sj79UiSZ1qG/o3/u/tgy5BkoZObUP/SOc3/kuS6hv6kqTXMvQlqUEMfUlqEENfkhrE0JekBjH0JalBDH1JahBDX5IaxNCXpAYx9CWpQQx9SWoQQ1+SGsTQl6QGMfQlqUEMfUlqEENfkhrE0JekBjH0JalBKoV+RFwRETsiYiwi1nZZvjAiNpTlWyJiacfycyNiX0T8+/6ULUk6Hj1DPyLmAeuAK4HlwNURsbyj2zXAnsw8H7gFuLlj+S3Al2ZeriRpJqoc6a8AxjJzZ2YeAO4EVnb0WQncXqY3ApdGRABExC8DO4Ht/SlZknS8qoT+WcBzbfPjpa1rn8w8BOwFFkfEqcBHgBtnXqokaaaqhH50acuKfW4EbsnMfcd8gohrI2I0IkYnJiYqlCRJOh7zK/QZB85pmz8b2DVFn/GImA8sAnYD7wJWRcQngdOAIxGxPzM/075yZq4H1gOMjIx0vqBIkvqkSuhvBS6IiGXAd4DVwPs7+mwC1gAPAKuA+zMzgV+Y7BAR/xHY1xn4kqQTp+fpnXKO/jpgM/A4cFdmbo+ImyLiqtLtVlrn8MeA64HXXNY5CM//YP+gS5CkoRKtA/LhMTIykqOjo8e17tK19xw1/3fOfBNf+tAvTNFbkuojIrZl5kivfrX+RO7EDz3Sl6R2tQ59SdLRDH1JapBah/6QvV0hSQNX69CXJB3N0JekBql16L/40oFBlyBJQ6XWoS9JOpqhL0kNYuhLUoMY+pLUIIa+JDWIoS9JDWLoS1KDGPqS1CCGviQ1iKEvSQ1i6EtSg9Q+9IftdpCSNEi1D/0vP/a9QZcgSUOj9qG/a6/3yZWkSbUPfUnS3zL0JalBDH1JapDah/5//YtvD7oESRoatQr9n3jbG1/T9oP9hwZQiSQNp1qF/pI3Lhx0CZI01GoV+q+LGHQJkjTUahb63dsPHT5yYguRpCFVq9A//y1v6Nr+kc8/eoIrkaThVKvQv/ydb+va/vkHx09wJZI0nCqFfkRcERE7ImIsItZ2Wb4wIjaU5VsiYmlp/8WI2BYRj5afl/S3fEnSdPQM/YiYB6wDrgSWA1dHxPKObtcAezLzfOAW4ObS/gLwTzPzp4A1wOf6VbgkafqqHOmvAMYyc2dmHgDuBFZ29FkJ3F6mNwKXRkRk5kOZuau0bwdOiohZu67Si3ck6diqhP5ZwHNt8+OlrWufzDwE7AUWd/T558BDmflK5xNExLURMRoRoxMTE1VrlyRNU5XQ73b83HlnkmP2iYh30jrl8xvdniAz12fmSGaOLFmypEJJU/FQX5KOpUrojwPntM2fDeyaqk9EzAcWAbvL/NnAnwIfyMwnZ1rwsUx1nb4kqaVK6G8FLoiIZRGxAFgNbOros4nWG7UAq4D7MzMj4jTgHuCGzPxav4qeyrE+kTv69O7ZfnpJGno9Q7+co78O2Aw8DtyVmdsj4qaIuKp0uxVYHBFjwPXA5GWd1wHnA/8hIh4uj7f0fRTFsd7I/cMHnpmtp5WkOWN+lU6ZeS9wb0fbx9qm9wPv67LeJ4BPzLDGyo51pH/oiF/FIEm1+kTusRw63PnesyQ1T61C/1hH+oePGPqSVKvQ98NZknRstQr9Yx3p73vFO2hJUq1C/1h3ztrylJdsSlKtQv+MUxcMugRJGmq1Cn1J0rEZ+pLUIIa+JDVIo0L/Jz++mQ1bnx10GZI0MI0K/X2vHOLjm7YPugxJGphGhT7A/oNH2H/w8KDLkKSBaFzoA/zP//f0oEuQpIFoZOjvefnAoEuQpIFoZOh/9i93DroESRqIRoa+JDWVoS9JDWLoS1KD1C70T1kwr1K/F/e9MsuVSNLwqV3o/9+PXFKp32/e8SB7f3RwlquRpOFSu9Cv+vXK33h6N3/3xj/nie//cJYrkqThUbvQn67fuufxQZcgSSdM40P//3x7gj/f/r1BlyFJJ0TjQx/g2s9t4+DhI4MuQ5JmnaFfvP/3vz7oEiRp1hn6xdan93D9hocHXYYkzSpDv80XHvoOmTnoMiRp1hj6Hf5k2/igS5CkWWPod/jwxkf48mNezSOpnmoZ+n/14X80o/U/eMc2vrv3R32qRpKGR6XQj4grImJHRIxFxNouyxdGxIayfEtELG1bdkNp3xERl/ev9Kmdc8YpM97Gr976jT5UIknDpWfoR8Q8YB1wJbAcuDoilnd0uwbYk5nnA7cAN5d1lwOrgXcCVwD/rWxv1l147mkzWn/s+X2MfOI+lq69h+d/uL9PVUnSYFU50l8BjGXmzsw8ANwJrOzosxK4vUxvBC6NiCjtd2bmK5n5FDBWtjfrNn7w52a8jRf2tW6ruOI/fYUP3fkQd3z9GQDufmQXY8/vA+Avvvl9Pn3ft/nTh8b51OYdjD69m4OHj7D35YMc8gNfkobM/Ap9zgKea5sfB941VZ/MPBQRe4HFpf3rHeueddzVTsPrXhf84a+v4AO39ec0zRcf3sUXH97F7//VTp558WUAzn/LG14N/0mf+erYlNtYuvgUni7rAvzY4lNYMK+Wb6tIOg7v+fElfPSXOk+k9FeV0I8ubZ0Xs0/Vp8q6RMS1wLUA5557boWSqnn3O5bwwA2XcPF/vr9v2zxz0Uk88+LLnHPGybzjra8NfYBFJ7++69c2/8Tb3nRU6L990cmcfurr+1abpLntrW86adafo0rojwPntM2fDeyaos94RMwHFgG7K65LZq4H1gOMjIz09dNRZy46maf/yy/1c5OSNGdVObewFbggIpZFxAJab8xu6uizCVhTplcB92fro62bgNXl6p5lwAWAl8VI0oD0PNIv5+ivAzYD84DbMnN7RNwEjGbmJuBW4HMRMUbrCH91WXd7RNwFfBM4BPzrzDw8S2ORJPUQw/ZdMyMjIzk6OjroMiRpTomIbZk50qufl45IUoMY+pLUIIa+JDWIoS9JDWLoS1KDDN3VOxExATwzg028GXihT+XMBU0bLzjmpnDM0/NjmbmkV6ehC/2ZiojRKpct1UXTxguOuSkc8+zw9I4kNYihL0kNUsfQXz/oAk6wpo0XHHNTOOZZULtz+pKkqdXxSF+SNIXahH6vm7fPJRFxTkR8NSIej4jtEfGh0n5GRNwXEU+Un6eX9oiI3y1jfyQiLmzb1prS/4mIWDPVcw6DiJgXEQ9FxN1lfllEbCm1byhf7U35qu4NZbxbImJp2zZuKO07IuLywYykmog4LSI2RsS3yr6+uAH7+N+Wf9OPRcQfR8RJddvPEXFbRDwfEY+1tfVtv0bEz0bEo2Wd342IbjermlpmzvkHra98fhI4D1gA/DWwfNB1zWA8ZwIXluk3At+mdVP6TwJrS/ta4OYy/V7gS7TuVHYRsKW0nwHsLD9PL9OnD3p8xxj39cAfAXeX+buA1WX694DfLNP/Cvi9Mr0a2FCml5d9vxBYVv5NzBv0uI4x3tuBf1mmFwCn1Xkf07pV6lPAyW3799fqtp+BdwMXAo+1tfVtv9K6J8nFZZ0vAVdOq75B/4L69Eu+GNjcNn8DcMOg6+rj+L4I/CKwAziztJ0J7CjTnwWubuu/oyy/GvhsW/tR/YbpQeuual8BLgHuLv+gXwDmd+5jWvd2uLhMzy/9onO/t/cbtgfwphKA0dFe5308eS/tM8p+uxu4vI77GVjaEfp92a9l2bfa2o/qV+VRl9M73W7efkJuwD7byp+0PwNsAd6amd8FKD/fUrpNNf659Hv5HeDDwJEyvxj4m8w8VObba391XGX53tJ/Lo33PGAC+INySut/RMSp1HgfZ+Z3gE8BzwLfpbXftlHv/TypX/v1rDLd2V5ZXUK/0g3Y55qIeAPweeDfZOYPjtW1S1vlG9MPWkT8E+D5zNzW3tyla/ZYNifGW8yndQrgv2fmzwAv0fqzfypzfszlPPZKWqdk3g6cClzZpWud9nMv0x3jjMdel9CvdAP2uSQiXk8r8P9XZn6hNH8/Is4sy88Eni/tU41/rvxe/gFwVUQ8DdxJ6xTP7wCnRcTkLT3ba391XGX5Ilq36Zwr44VWreOZuaXMb6T1IlDXfQzwj4GnMnMiMw8CXwB+jnrv50n92q/jZbqzvbK6hH6Vm7fPGeXd+FuBxzPz022L2m9Av4bWuf7J9g+UKwEuAvaWPyE3A5dFxOnlKOuy0jZUMvOGzDw7M5fS2nf3Z+avAF8FVpVuneOd/D2sKv2ztK8uV30sAy6g9abX0MnM7wHPRcSPl6ZLad1Lupb7uHgWuCgiTin/xifHXNv93KYv+7Us+2FEXFR+hx9o21Y1g37Do49vnLyX1lUuTwIfHXQ9MxzLz9P6k+0R4OHyeC+t85lfAZ4oP88o/QNYV8b+KDDStq1fB8bK418MemwVxv4e/vbqnfNo/WceA/4EWFjaTyrzY2X5eW3rf7T8HnYwzasaBjDWvweMlv38Z7Su0qj1PgZuBL4FPAZ8jtYVOLXaz8Af03rP4iCtI/Nr+rlfgZHy+3sS+AwdFwP0eviJXElqkLqc3pEkVWDoS1KDGPqS1CCGviQ1iKEvSQ1i6EtSgxj6ktQghr4kNcj/By4E6QpVViemAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f262904208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.26| 0.05| 0.04| 0.00|\n",
      "---------------------------\n",
      " 0.12| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.28| 0.07| 0.05| 0.11|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 1.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Increasing t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "it: 0\n",
      "it: 2000\n",
      "it: 4000\n",
      "it: 6000\n",
      "it: 8000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f264ae9dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update counts:\n",
      "---------------------------\n",
      " 0.36| 0.03| 0.02| 0.00|\n",
      "---------------------------\n",
      " 0.07| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.03| 0.03| 0.14|\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "from td0_prediction import random_action\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # NOTE: if we use the standard grid, there's a good chance we will end up with\n",
    "  # suboptimal policies\n",
    "  # e.g.\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   R* |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   R  |   U  |   L  |\n",
    "  # since going R at (1,0) (shown with a *) incurs no cost, it's OK to keep doing that.\n",
    "  # we'll either end up staying in the same spot, or back to the start (2,0), at which\n",
    "  # point we whould then just go back up, or at (0,0), at which point we can continue\n",
    "  # on right.\n",
    "  # instead, let's penalize each movement so the agent will find a shorter route.\n",
    "  #\n",
    "  # grid = standard_grid()\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # no policy initialization, we will derive our policy from most recent Q\n",
    "\n",
    "  # initialize Q(s,a)\n",
    "  Q = {}\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      Q[s][a] = 0\n",
    "\n",
    "  # let's also keep track of how many times Q[s] has been updated\n",
    "  update_counts = {}\n",
    "  update_counts_sa = {}\n",
    "  for s in states:\n",
    "    update_counts_sa[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "      update_counts_sa[s][a] = 1.0\n",
    "\n",
    "  # repeat until convergence\n",
    "  t = 3.0\n",
    "  deltas = []\n",
    "  for it in range(10000):\n",
    "    if it % 100 == 0:\n",
    "      t += 1e-2\n",
    "    if it % 2000 == 0:\n",
    "      print(\"it:\", it)\n",
    "\n",
    "    # instead of 'generating' an epsiode, we will PLAY\n",
    "    # an episode within this loop\n",
    "    s = (2, 0) # start state\n",
    "    grid.set_state(s)\n",
    "\n",
    "    # the first (s, r) tuple is the state we start in and 0\n",
    "    # (since we don't get a reward) for simply starting the game\n",
    "    # the last (s, r) tuple is the terminal state and the final reward\n",
    "    # the value for the terminal state is by definition 0, so we don't\n",
    "    # care about updating it.\n",
    "    a, _ = max_dict(Q[s])\n",
    "    biggest_change = 0\n",
    "    while not grid.game_over():\n",
    "      a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "      # random action also works, but slower since you can bump into walls\n",
    "      # a = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "      r = grid.move(a)\n",
    "      s2 = grid.current_state()\n",
    "\n",
    "      # adaptive learning rate\n",
    "      alpha = ALPHA / update_counts_sa[s][a]\n",
    "      update_counts_sa[s][a] += 0.005\n",
    "\n",
    "      # we will update Q(s,a) AS we experience the episode\n",
    "      old_qsa = Q[s][a]\n",
    "      # the difference between SARSA and Q-Learning is with Q-Learning\n",
    "      # we will use this max[a']{ Q(s',a')} in our update\n",
    "      # even if we do not end up taking this action in the next step\n",
    "      a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "      Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "      biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "      # we would like to know how often Q(s) has been updated too\n",
    "      update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "      # next state becomes current state\n",
    "      s = s2\n",
    "     \n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # determine the policy from Q*\n",
    "  # find V* from Q*\n",
    "  policy = {}\n",
    "  V = {}\n",
    "  for s in grid.actions.keys():\n",
    "    a, max_q = max_dict(Q[s])\n",
    "    policy[s] = a\n",
    "    V[s] = max_q\n",
    "\n",
    "  # what's the proportion of time we spend updating each part of Q?\n",
    "  print(\"update counts:\")\n",
    "  total = np.sum(list(update_counts.values()))\n",
    "  for k, v in update_counts.items():\n",
    "    update_counts[k] = float(v) / total\n",
    "  print_values(update_counts, grid)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
